From PPO to GRPO

Reading this tutorial requires no prior knowledge of Reinforcement Learning — the focus is on being friendly and approachable!
Estimated reading time: 15 minutes+
If you find any mistakes, please contact Lin Jinghao for corrections.
Target audience: learners with a deep learning background + knowledge of advanced mathematics.

Recommended Introductory Notes

Highly recommended: the Large Model Preference Alignment series!

- [Mengyuan: Illustrated RLHF for Large Models — PPO Principles and Source Code Explained for Everyone](https://zhuanlan.zhihu.com/p/677607581)
- [Mengyuan: RLHF-PPO Theory Explained for Everyone](https://zhuanlan.zhihu.com/p/7461863937)
- [Mengyuan: Illustrated OpenRLHF with Ray-Based Distributed Training Process](https://zhuanlan.zhihu.com/p/12871616401)
- [Mengyuan: DPO Mathematical Principles Explained for Everyone](https://zhuanlan.zhihu.com/p/721073733)
- [Mengyuan: OpenAI o1 Tech Insights (1): Overall Framework, Using Test-Time Scaling Law to Improve Logical Reasoning](https://zhuanlan.zhihu.com/p/773907223)
- [Mengyuan: OpenAI o1 Tech Insights (2): Enhancing Reasoning with MCTS (Code-Based Walkthrough](https://zhuanlan.zhihu.com/p/864190605)
- [Mengyuan: OpenAI o1 Tech Insights (3): How to Give Models Self-Correction Abilities](https://zhuanlan.zhihu.com/p/905620136)
- [Mengyuan: Notes on Understanding DeepSeek-R1](https://zhuanlan.zhihu.com/p/19843230707)
